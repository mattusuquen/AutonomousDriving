{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PolicyNetwork import PolicyNetwork\n",
    "from ValueNetwork import ValueNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using gpu\n",
    "device = torch.device('cuda' if torch.cuda_is_available() else 'cpu')\n",
    "\n",
    "accel_policy = PolicyNetwork()\n",
    "turn_policy = PolicyNetwork()\n",
    "value_function = ValueNetwork()\n",
    "\n",
    "accel_policy = torch.load('acceleration_network.pth')\n",
    "turn_policy = torch.load('turn_network.pth')\n",
    "value_function = torch.load('value_network.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and read trajectory csv file\n",
    "trajectories_file = 'trajectories.csv'\n",
    "trajectories = pd.read_csv(trajectories_file)\n",
    "\n",
    "# Store trajectory values into state, action, reward vectors\n",
    "states = trajectories.iloc[:, :-3].values\n",
    "actions = trajectories.iloc[:, -3:-1].values\n",
    "rewards = trajectories.iloc[:, -1:].values\n",
    "\n",
    "# Calculate reward-to-go\n",
    "reward_to_go = np.array([[reward[0]] for reward in rewards])\n",
    "for i in range(len(reward_to_go)-2,-1,-1): reward_to_go[i][0] += reward_to_go[i][0]\n",
    "\n",
    "#Store in dataset\n",
    "dataset = TensorDataset(states, reward_to_go)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(value_function.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "\n",
    "losses = []\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    for inputs_batch, outputs_batch in dataloader:\n",
    "        # Move batches to GPU\n",
    "        inputs_batch, outputs_batch = inputs_batch.to(device), outputs_batch.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = value_function(inputs_batch)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_function(predictions, outputs_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Policy Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_optimizer = optim.Adam([accel_policy.parameters(),turn_policy.parameters()],lr=0.0001)\n",
    "value_optimizer = optim.Adam(value_function.parameters(),lr=0.0001)\n",
    "\n",
    "epsilon = 0.2 # Clipping factor\n",
    "\n",
    "for epoch in range(epoch):\n",
    "\n",
    "    # Clone current policy and value networks\n",
    "    accel_policy_old = accel_policy.clone()\n",
    "    turn_policy_old = turn_policy.clone()\n",
    "    value_function_old = value_function.clone()\n",
    "\n",
    "    values = value_function(states)\n",
    "\n",
    "    # Freeze clone networks\n",
    "    for param in accel_policy_old.parameters(): param.requires_grad = False\n",
    "    for param in turn_policy_old.parameters(): param.requires_grad = False\n",
    "    for param in value_function_old.parameters(): param.requires_grad = False\n",
    "\n",
    "    # Calculate advantages\n",
    "    advantages = reward_to_go - value_function_old(states).numpy()\n",
    "\n",
    "    # Get old policy distribution from policy networks\n",
    "    accel_mean_old, accel_stdev_old = accel_policy_old(states)[:,0],accel_policy_old(states)[:,1]\n",
    "    turn_mean_old, turn_stdev_old = turn_policy_old(states)[:,0],turn_policy_old(states)[:,1]\n",
    "    # Get new policy distribution from policy networks\n",
    "    accel_mean, accel_stdev = accel_policy(states)[:,0],accel_policy(states)[:,1]\n",
    "    turn_mean, turn_stdev = turn_policy(states)[:,0],turn_policy(states)[:,1]\n",
    "\n",
    "    # Get acceleration and turn from actions taken\n",
    "    accelerations = actions[:,0]\n",
    "    turns = actions[:,1]\n",
    "\n",
    "    # Calulate probability given action using density function\n",
    "    pi = (1 / (accel_stdev * torch.sqrt(torch.tensor(2 * torch.pi)))) * torch.exp(-0.5 * ((accelerations - accel_mean) / accel_stdev) ** 2) * (1 / (turn_stdev * torch.sqrt(torch.tensor(2 * torch.pi)))) * torch.exp(-0.5 * ((turns - turn_mean) / turn_stdev) ** 2)\n",
    "    pi_old = (1 / (accel_stdev_old * torch.sqrt(torch.tensor(2 * torch.pi)))) * torch.exp(-0.5 * ((accelerations - accel_mean_old) / accel_stdev_old) ** 2) * (1 / (turn_stdev_old * torch.sqrt(torch.tensor(2 * torch.pi)))) * torch.exp(-0.5 * ((turns - turn_mean_old) / turn_stdev_old) ** 2)\n",
    "    \n",
    "    # Calculate probability ratios\n",
    "    ratios = pi / pi_old\n",
    "\n",
    "    # Compute PPO loss\n",
    "    surr1 = ratios * advantages\n",
    "    surr2 = torch.clamp(ratios, 1 - epsilon, 1 + epsilon) * advantages # Clipping\n",
    "    ppo_loss = -min(surr1,surr2).mean()\n",
    "\n",
    "    # Update policy net\n",
    "    policy_optimizer.zero_grad()\n",
    "    ppo_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "\n",
    "    # Update value network\n",
    "    value_loss = nn.MSELoss()(values,reward_to_go)\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    value_optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
