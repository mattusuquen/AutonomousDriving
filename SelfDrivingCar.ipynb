{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PolicyNetwork import PolicyNetwork\n",
    "from ValueNetwork import ValueNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using gpu\n",
    "device = torch.device('cuda' if torch.cuda_is_available() else 'cpu')\n",
    "\n",
    "accel_policy = PolicyNetwork()\n",
    "turn_policy = PolicyNetwork()\n",
    "value_function = ValueNetwork()\n",
    "\n",
    "accel_policy = torch.load('acceleration_network.pth')\n",
    "turn_policy = torch.load('turn_network.pth')\n",
    "value_function = torch.load('value_network.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and read trajectory csv file\n",
    "trajectories_file = 'trajectories.csv'\n",
    "trajectories = pd.read_csv(trajectories_file)\n",
    "\n",
    "# Store trajectory values into state, action, reward vectors\n",
    "states = trajectories.iloc[:, :-3].values\n",
    "actions = trajectories.iloc[:, -3:-1].values\n",
    "rewards = trajectories.iloc[:, -1:].values\n",
    "\n",
    "# Calculate reward-to-go\n",
    "reward_to_go = np.array([[reward[0]] for reward in rewards])\n",
    "for i in range(len(reward_to_go)-2,-1,-1): reward_to_go[i][0] += reward_to_go[i][0]\n",
    "\n",
    "#Store in dataset\n",
    "dataset = TensorDataset(states, reward_to_go)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(value_function.parameters(), lr=0.001, weight_decay=0.001)\n",
    "\n",
    "losses = []\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    for inputs_batch, outputs_batch in dataloader:\n",
    "        # Move batches to GPU\n",
    "        inputs_batch, outputs_batch = inputs_batch.to(device), outputs_batch.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = value_function(inputs_batch)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_function(predictions, outputs_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
